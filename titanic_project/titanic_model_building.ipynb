{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Titanic Competition - Model Building, Testing, and Selection\n",
    "- Problem Description: The sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew. While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others. In this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc). \n",
    "- Author: Kimberly Gaddie\n",
    "- Date Last Updated: 17 May 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import (StandardScaler, MinMaxScaler, \n",
    "                                   LabelEncoder, OneHotEncoder)\n",
    "\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier,\n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier,\n",
    "                              VotingClassifier)\n",
    "\n",
    "from sklearn.model_selection import (GridSearchCV, cross_val_score, cross_val_predict,\n",
    "                                     StratifiedKFold, learning_curve, train_test_split)\n",
    "\n",
    "from sklearn.metrics import (confusion_matrix, roc_curve, auc, roc_auc_score, \n",
    "                             accuracy_score, f1_score, explained_variance_score)\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from yellowbrick.model_selection import FeatureImportances\n",
    "import xgboost as xgb\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\"notebook.output.textLineLimit\": 500\n",
    "\n",
    "df_full = read.csv('df_full_featured.csv')\n",
    "df_train = read.csv('train.csv')\n",
    "df_test = read.csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "- Unpack the full dataset back to test & train\n",
    "- Feature Selection\n",
    "- HyperParameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known = df_full[df_full['df_type'] == 'Train']\n",
    "val = df_full[df_full['df_type'] == 'Test']\n",
    "print(f'{len(known)} Training Records & {len(val)} Validation Records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 5254 # Reproducability!!!\n",
    "\n",
    "# Usually use 75/25 or 80/20 split... But w/ low n... Let's do 85/15\n",
    "    # This will probably bias our test metrics lower 'indicating' a 'worse'\n",
    "    # performing model (less n, higher probability of outlier / fringe case\n",
    "    # scenarios not learned by model...)\n",
    "train, test = train_test_split(known, test_size=0.15, random_state=random_state)\n",
    "\n",
    "print(f'''{len(train)} Training Records & {len(test)} Testing Records.\n",
    "      Split of {len(train) / len(known)} | {len(test) / len(known)}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with all of these... We should eventually pare the list down\n",
    "\n",
    "features = ['female', 'male', 'Fare', 'New Fare', 'Pclass', 'is_minor', \n",
    "            'is_group', 'Total Family', 'Upper', 'Miss', 'Mrs', 'fare_divider', \n",
    "            'Embarked_S', 'Embarked_Q', 'Embarked_C', 'cabin_B', 'cabin_C', 'cabin_D',\n",
    "            'cabin_G', 'cabin_F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate train features and label \n",
    "Y_train = train[\"Survived\"].astype(int)\n",
    "X_train = train[features]\n",
    "\n",
    "y_test = test['Survived']\n",
    "test = test[features]\n",
    "\n",
    "# Cross validate model with Kfold stratified cross val\n",
    "folds = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 'Out of the Box' solutions\n",
    "- We can narrow down model types by Identifying the better performing models\n",
    "- This doesn't include any hyperparameter tuning!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = []\n",
    "cv_results = []\n",
    "cv_means = []\n",
    "cv_std = []\n",
    "\n",
    "model_types.append(KNeighborsClassifier())\n",
    "model_types.append(DecisionTreeClassifier(random_state=random_state))\n",
    "model_types.append(RandomForestClassifier(random_state=random_state))\n",
    "model_types.append(SVC(random_state=random_state))\n",
    "model_types.append(GradientBoostingClassifier(random_state=random_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_types:\n",
    "    cv_results.append(cross_val_score(model, X_train, Y_train, scoring = 'accuracy', cv=folds, n_jobs=4))\n",
    "    \n",
    "for result in cv_results:\n",
    "    cv_means.append(np.median(result))\n",
    "    cv_std.append(np.std(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance = pd.DataFrame({\n",
    "    'Cross_Val_Median': cv_means,\n",
    "    'Cross_Val_Errors': cv_std,\n",
    "    'Model_Types': ['KNeighbors',\n",
    "                   'DecisionTree',\n",
    "                   'RandomForest',\n",
    "                   'SVC',\n",
    "                   'GradientBoost']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotted = sns.barplot('Cross_Val_Median', 'Model_Types', data=model_performance, orient='h', **{'xerr': cv_std})\n",
    "plotted.set_xlabel('Median Model Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HyperParameter Tuning\n",
    "- Random Forest (I just like it, but might have better luck w/ Decision Tree\n",
    "    or Gradient Boosting)\n",
    "- GB (most likely will switch to xgboost package here, I like the \n",
    "    implementation / api better than in sklearn)\n",
    "- Decision Tree\n",
    "- KNN (MAYBE -- I am super unconvinced it will actually provide meaningful\n",
    "    value)\n",
    "- Can we ensemble these together..? They may provide improved performance in a\n",
    "    grouped competitive voting situation..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest\n",
    "- We'll use a grid search cross validation method... Not the fastest, cpu \n",
    "    intensive, but easiest heuristic to implement. \n",
    "- We should consider other methods - Optuna, Baysian, etc...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize decision tree\n",
    "# Not well defined in documentation, let's try both\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Number of trees in forest... Must be careful!\n",
    "    # Exceeding observations = over-fitting and possible 'perfect' predition\n",
    "n_estimators = [35, 40, 45, 50, 100, 200, 300]\n",
    "\n",
    "# Maximum depth of the tree (could use None (default), but overfitting...)\n",
    "max_depth = [5, 8, 10, 13, 15, 18, 20, 25, 30]\n",
    "\n",
    "# Threshold for creating a new split\n",
    "min_samples_split = [2, 5, 10, 15, 100]\n",
    "\n",
    "# Min samples required to keep a leaf\n",
    "min_samples_leaf = [1, 2, 5, 10]\n",
    "\n",
    "# Function used to measure quality of a split\n",
    "criterion=['gini','entropy']\n",
    "\n",
    "\n",
    "forest = RandomForestClassifier(random_state = random_state) \n",
    "\n",
    "hyperF = dict(bootstrap = bootstrap,\n",
    "              n_estimators = n_estimators,\n",
    "              max_depth = max_depth,  \n",
    "              min_samples_split = min_samples_split, \n",
    "              min_samples_leaf = min_samples_leaf,\n",
    "              criterion = criterion)\n",
    "\n",
    "gridF = GridSearchCV(forest, hyperF, cv = folds, verbose = 1, \n",
    "                      n_jobs = -1)\n",
    "bestF = gridF.fit(X_train, Y_train)\n",
    "\n",
    "# What did we determine the best estimator was?\n",
    "# We can pickle this for later?\n",
    "# If we use MLFlow this can be an artifact\n",
    "print(bestF.best_params_)\n",
    "\n",
    "# What did we determine the best parameters were?\n",
    "# We can pickle this for later?\n",
    "# If we use MLFlow this can be an artifact\n",
    "print(bestF.best_estimator_)\n",
    "\n",
    "y_predF = bestF.predict(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How did our new model do?\n",
    "# AUC - Area Under the Curve\n",
    "# F1 Score - Calculated from percision and recall\n",
    "    ## Precision = correctly predicted positives / total predicted positives\n",
    "    ## Recall = correctly predicted positives / total true positives\n",
    "# Accuracy - (TP + TN)/(TP + TN + FP + FN)\n",
    "\n",
    "y_scores = bestF.predict_proba(test)[:,1]\n",
    "fpr, tpr, thres = roc_curve(y_test,  y_scores)\n",
    "\n",
    "print('AUC: ', auc(fpr, tpr))\n",
    "\n",
    "print('F1: ',f1_score(y_test, y_predF))\n",
    "\n",
    "print('Accuracy: ',accuracy_score(y_test, y_predF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we visualize this? \n",
    "\n",
    "# compute true positive rate and false positive rate\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_scores)\n",
    "\n",
    "# Define function to plot them\n",
    "def plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n",
    "    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'r', linewidth=4)\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate (FPR)', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate (TPR)', fontsize=16)\n",
    "\n",
    "# Set figure size    \n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Run function\n",
    "plot_roc_curve(false_positive_rate, true_positive_rate)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree\n",
    "- We'll use a grid search cross validation method... Not the fastest, cpu \n",
    "    intensive, but easiest heuristic to implement. \n",
    "- We should consider other methods - Optuna, Baysian, etc...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we don't want to run this in isolation, we could steal these from\n",
    "    # above\n",
    "\n",
    "# Function used to measure quality of a split\n",
    "criterion=['gini','entropy']\n",
    "\n",
    "# Maximum depth of the tree (could use None (default), but overfitting...)\n",
    "max_depth = [5, 8, 10, 13, 15, 18, 20, 25, 30]\n",
    "\n",
    "# Threshold for creating a new split\n",
    "min_samples_split = [2, 5, 10, 15, 100]\n",
    "\n",
    "# Min samples required to keep a leaf\n",
    "min_samples_leaf = [1, 2, 5, 10]\n",
    "\n",
    "param_grid = {'criterion': criterion,\n",
    "              'max_depth': max_depth,\n",
    "              'min_samples_leaf': min_samples_leaf,\n",
    "              'min_samples_split': min_samples_split}\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=random_state)\n",
    "gs = GridSearchCV(tree,param_grid,cv=folds)\n",
    "gs.fit(X_train,Y_train)\n",
    "\n",
    "best = gs.best_estimator_\n",
    "print(best)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred2 = best.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = best.predict_proba(test)[:,1]\n",
    "fpr, tpr, thres = roc_curve(y_test,  y_pred_proba)\n",
    "\n",
    "print('AUC: ', auc(fpr, tpr))\n",
    "\n",
    "print('F1: ',f1_score(y_test, y_pred2))\n",
    "\n",
    "print('Accuracy: ',accuracy_score(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new matplotlib figure\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "fts = FeatureImportances(best, ax=ax)\n",
    "fts.fit(X_train, Y_train)\n",
    "fts.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost\n",
    "-Switch to xgboost package, as noted earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Convert into data structure for xgb\n",
    "data_dmatrix = xgb.DMatrix(data=X_train,label=Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [5, 10, 20, 50, 100, 200, 300]\n",
    "max_depth = [3, 6, 9, 12]\n",
    "min_child_weight = [1, 5, 10]\n",
    "gamma = [0, 1, 2]\n",
    "subsample = [0.5, 0.75, 1]\n",
    "colsample_bytree = [0.5, 1]\n",
    "reg_alpha = [0.001, 0.01, 0.1]\n",
    "reg_lambda = [0.001, 0.01, 0.1]\n",
    "scale_pos_weight = [1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': n_estimators,\n",
    "              'max_depth': max_depth,\n",
    "              'min_child_weight': min_child_weight,\n",
    "              'gamma': gamma,\n",
    "              'subsample': subsample,\n",
    "              'colsample_bytree': colsample_bytree,\n",
    "              'reg_lambda': reg_lambda,\n",
    "              'reg_alpha': reg_alpha,\n",
    "              'scale_pos_weight': scale_pos_weight}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch = GridSearchCV(\n",
    "        estimator=xgb.XGBClassifier(\n",
    "            objective = \"binary:logistic\",\n",
    "            nthread = 1,\n",
    "            seed = random_state,\n",
    "            verbosity = 1,\n",
    "            metric = 'rmse'\n",
    "        ),\n",
    "        param_grid=param_grid,\n",
    "        error_score=0,\n",
    "        n_jobs=32,\n",
    "        verbose=1,\n",
    "        return_train_score=True,\n",
    "        iid=False,\n",
    "        cv=folds\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch.fit(X_train, Y_train)\n",
    "\n",
    "print(f'Mean test scores of Grid search: {gsearch.cv_results_[\"mean_test_score\"].mean()}')\n",
    "print(f'Selected hyper parameters from grid search: {gsearch.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how we did\n",
    "y_pred3 = gsearch.predict(test)\n",
    "y_pred3_score = gsearch.predict_proba(test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thres = roc_curve(y_test,  y_pred3_score)\n",
    "\n",
    "print('AUC: ', auc(fpr, tpr))\n",
    "\n",
    "print('F1: ',f1_score(y_test, y_pred3))\n",
    "\n",
    "print('Accuracy: ',accuracy_score(y_test, y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch.best_score_\n",
    "xgb_best = gsearch.best_estimator_\n",
    "print(xgb_best)\n",
    "\n",
    "xgb.plot_importance(xgb_best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2ae1dbaf558097aaa66a0f40fd3ab3ed59b44f80029a319580321f6355551186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
